{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2u6GwIrq3fog",
    "outputId": "f14d9b7d-d11b-4fd5-feee-4be09cc17cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pgbRHkKQwGK",
    "outputId": "3848a41d-27fd-41ea-8343-93cc61014dc8"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/mod_dataset_heartbeat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Data augmentation and preprocessing\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m     28\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[1;32m     29\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Increased augmentation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# 20% of the data will be used for validation\u001b[39;00m\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/mod_dataset_heartbeat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     41\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     42\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     43\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/mod_dataset_heartbeat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     48\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DirectoryIterator(\n\u001b[1;32m   1139\u001b[0m         directory,\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1141\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   1142\u001b[0m         color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[1;32m   1143\u001b[0m         keep_aspect_ratio\u001b[38;5;241m=\u001b[39mkeep_aspect_ratio,\n\u001b[1;32m   1144\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[1;32m   1145\u001b[0m         class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[1;32m   1146\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[1;32m   1147\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1148\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m   1149\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1150\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[1;32m   1151\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[1;32m   1152\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[1;32m   1153\u001b[0m         follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[1;32m   1154\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[1;32m   1155\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[1;32m   1156\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m   1157\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/mod_dataset_heartbeat'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MobileNetV2 with pretrained ImageNet weights and exclude the top layers\n",
    "def create_transfer_model(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze some of the layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom layers on top of the base model\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))  # L2 regularization\n",
    "    model.add(layers.Dropout(0.5))  # Add dropout to reduce overfitting\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,  # Increased augmentation\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 20% of the data will be used for validation\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/content/drive/MyDrive/mod_dataset_heartbeat',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/content/drive/MyDrive/mod_dataset_heartbeat',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "model = create_transfer_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),  # Lower learning rate\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint = ModelCheckpoint('best_transfer_model.keras', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,  # Increase epochs for better training\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint, reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Fine-tune the model by unfreezing more layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:  # Unfreeze last 50 layers for fine-tuning\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a very low learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),  # Fine-tune with a smaller learning rate\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  # Few additional epochs for fine-tuning\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint, reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(fine_tune_history.history['accuracy'], label='Fine-tuned Training Accuracy')\n",
    "plt.plot(fine_tune_history.history['val_accuracy'], label='Fine-tuned Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(fine_tune_history.history['loss'], label='Fine-tuned Training Loss')\n",
    "plt.plot(fine_tune_history.history['val_loss'], label='Fine-tuned Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
